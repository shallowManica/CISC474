{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Taxi game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: gym[toy_text]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipykernel in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (6.17.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (6.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (7.28.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (1.5.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (23.2.0)\n",
      "Requirement already satisfied: packaging in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (21.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (7.0.6)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (5.1.1)\n",
      "Requirement already satisfied: nest-asyncio in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (1.5.5)\n",
      "Requirement already satisfied: psutil in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: appnope in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (0.1.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (0.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (65.5.0)\n",
      "Requirement already satisfied: decorator in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n",
      "Requirement already satisfied: pygments in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (2.11.2)\n",
      "Requirement already satisfied: backcall in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.18.1)\n",
      "Requirement already satisfied: entrypoints in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from packaging->ipykernel) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the libraries for the environment\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "%pip install pyvirtualdisplay\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE, MAE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "%pip install gym[toy_text]\n",
    "%pip install ipykernel\n",
    "from gym import envs\n",
    "import datetime\n",
    "import keras \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from time import sleep\n",
    "\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "# set up the environment\n",
    "ENV_NAME = \"Taxi-v3\"\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward at Episode 50: -114\n",
      "Average timesteps per episode: 8.378\n",
      "Total Reward at Episode 100: -35\n",
      "Average timesteps per episode: 13.151\n",
      "Total Reward at Episode 150: 11\n",
      "Average timesteps per episode: 16.056\n",
      "Total Reward at Episode 200: -3\n",
      "Average timesteps per episode: 17.713\n",
      "Total Reward at Episode 250: 4\n",
      "Average timesteps per episode: 18.641\n",
      "Total Reward at Episode 300: 4\n",
      "Average timesteps per episode: 19.534\n",
      "Total Reward at Episode 350: 8\n",
      "Average timesteps per episode: 20.313\n",
      "Total Reward at Episode 400: 6\n",
      "Average timesteps per episode: 21.052\n",
      "Total Reward at Episode 450: 8\n",
      "Average timesteps per episode: 21.734\n",
      "Total Reward at Episode 500: 12\n",
      "Average timesteps per episode: 22.4\n",
      "Total Reward at Episode 550: 5\n",
      "Average timesteps per episode: 23.065\n",
      "Total Reward at Episode 600: 9\n",
      "Average timesteps per episode: 23.738\n",
      "Total Reward at Episode 650: 7\n",
      "Average timesteps per episode: 24.405\n",
      "Total Reward at Episode 700: 5\n",
      "Average timesteps per episode: 25.058\n",
      "Total Reward at Episode 750: 8\n",
      "Average timesteps per episode: 25.719\n",
      "Total Reward at Episode 800: 7\n",
      "Average timesteps per episode: 26.395\n",
      "Total Reward at Episode 850: 12\n",
      "Average timesteps per episode: 27.094\n",
      "Total Reward at Episode 900: 7\n",
      "Average timesteps per episode: 27.759\n",
      "Total Reward at Episode 950: 7\n",
      "Average timesteps per episode: 28.403\n",
      "Total Reward at Episode 1000: 10\n",
      "Average timesteps per episode: 29.042\n",
      "Average reward over 1000 episodes: -15.746\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.]\n",
      "[-1.719 -1.719 -1.719 13.59  -9.    -9.   ]\n",
      "Time taken: 29042\n"
     ]
    }
   ],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS]) \n",
    "\n",
    "gamma = 0.9 # discount factor\n",
    "alpha = 0.9 # learning rate\n",
    "epsilon = 0.9\n",
    "\n",
    "total_epochs = 0\n",
    "\n",
    "eps =[]\n",
    "rewd = []\n",
    "\n",
    "for episode in range(1,1001):\n",
    "    done = False\n",
    "    total_rewd = 0 # total reward\n",
    "    obs = env.reset()\n",
    "    epochs = 0\n",
    "    steps = []\n",
    "\n",
    "    while done != True:\n",
    "      if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "      else:\n",
    "            # Exploit learned values\n",
    "            action = np.argmax(Q[obs]) \n",
    "            obs2, rew, done, info = env.step(action) \n",
    "            Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) \n",
    "\n",
    "            total_rewd +=  rew\n",
    "            obs = obs2   \n",
    "            epochs += 1\n",
    "\n",
    "    total_epochs += epochs\n",
    "\n",
    "    eps.append(episode)\n",
    "    rewd.append(total_rewd)\n",
    "    avgsteps = total_epochs / 1000\n",
    "\n",
    "    # print out the result for every 50 episodes\n",
    "    if episode % 50 == 0: \n",
    "      \n",
    "      print('Total Reward at Episode {}: {}'.format(episode,total_rewd))\n",
    "      print(f\"Average timesteps per episode: {avgsteps}\")\n",
    "      \n",
    "\n",
    "avgrewd = sum(rewd) / 1000\n",
    "\n",
    "print(\"Average reward over 1000 episodes:\", avgrewd)\n",
    "\n",
    "plt.plot(eps,rewd)\n",
    "plt.title(\"Rewards for Episodes over time\")\n",
    "plt.ylabel(\"Total Rewards\")\n",
    "plt.xlabel(\"Number of Episodes\")\n",
    "plt.show()\n",
    "\n",
    "# Generating Q-table\n",
    "print(Q[0])\n",
    "print(Q[499])\n",
    "\n",
    "print(\"Time taken:\", total_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Reward: 15\n"
     ]
    }
   ],
   "source": [
    "#following the policy to take actions delivering max value\n",
    "\n",
    "total_rewd=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    total_rewd = total_rewd + rew\n",
    "    rewd.append(total_rewd)\n",
    "    env.render()\n",
    "#Print the reward of these actions\n",
    "max_rewd = max(rewd)\n",
    "print(\"Reward: %r\" % max_rewd)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi-v3\n",
      "Action Space 6\n",
      "State Space 500\n",
      "Episode:   1000/1000 || Average reward: -814.16 || eps: 0.010000"
     ]
    }
   ],
   "source": [
    "#agent class\n",
    "class Agent:\n",
    "    def __init__(self, Q, nA=6):\n",
    "        self.Q = Q\n",
    "        self.nA = nA\n",
    "        self.step = self.monteCarloControlStep\n",
    "        self.alpha = 0.01 \n",
    "        self.gamma = 0.9  \n",
    "        self.episode = list()\n",
    "        \n",
    "    #action selection function\n",
    "    def selectAction(self, state, eps):\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(self.Q[state])\n",
    "        else:\n",
    "            return np.random.choice(self.nA)\n",
    "\n",
    "    #monte carlo control step function\n",
    "    def monteCarloControlStep(self, state, action, reward, next_state, done):\n",
    "\n",
    "        if done:\n",
    "            rewards = defaultdict(lambda: np.zeros(self.nA))\n",
    "            for history in reversed(self.episode):\n",
    "                state, action, reward = history\n",
    "                rewards[state][action] = reward + self.gamma * rewards[state][action]\n",
    "                self.Q[state][action] += self.alpha * (rewards[state][action] - self.Q[state][action])\n",
    "            self.episode.clear()\n",
    "        else:\n",
    "            self.episode.append((state, action, reward))\n",
    "\n",
    "#monte carlo control function\n",
    "def monteCarloControl(Q = defaultdict(lambda: np.zeros(action_size))):\n",
    "    agent = Agent(Q)\n",
    "    totalEps = 1000\n",
    "    sampleRewards = deque(maxlen=100)\n",
    "\n",
    "    for episodeI in range(totalEps):\n",
    "        state = env.reset()\n",
    "        eps = 0.01\n",
    "        sampleReward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.selectAction(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            sampleReward += reward\n",
    "            if done:\n",
    "                sampleRewards.append(sampleReward)\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        if episodeI >= 100:\n",
    "            rewardAvg = sum(sampleRewards) / len(sampleRewards)\n",
    "            episodeI += 1\n",
    "            print(f\"\\rEpisode: {episodeI:6d}/{totalEps} || Average reward: {rewardAvg:7.2f} || eps: {eps:.5f}\", end='')\n",
    "\n",
    "\n",
    "            \n",
    "env = gym.make(\"Taxi-v3\")\n",
    "print(env.spec.id)\n",
    "\n",
    "# Number of possible actions\n",
    "action_size = env.action_space.n\n",
    "print(f\"Action Space {env.action_space.n}\")\n",
    "\n",
    "# Number of possible states\n",
    "space_size = env.observation_space.n\n",
    "print(f\"State Space {env.observation_space.n}\")\n",
    "\n",
    "#function call\n",
    "monteCarloControl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global random seeds\n",
    "tf.random.set_seed(0)\n",
    "# set up the hyperparameter\n",
    "MEMORY_SIZE = 800_000\n",
    "GAMMA = 0.95\n",
    "ALPHA = 0.1\n",
    "NUM_STEPS_FOR_UPDATE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "Number of states: 500\n"
     ]
    }
   ],
   "source": [
    "# create tuple subclasses with named fields\n",
    "experience = namedtuple(\"Experience\", \n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "#Since gym libries already assign the actions and state for the Taxi game\n",
    "#We do not need to assign them again\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "print(\"Number of actions:\", action_size)\n",
    "print(\"Number of states:\",state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 128)               64128     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 97,926\n",
      "Trainable params: 97,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The network contains one input layer which instantiate a Keras tensor, \n",
    "# three hidden dense layer and the final dense layer should have the same size of action space\n",
    "model = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(action_size, activation=\"linear\")\n",
    "])\n",
    "target_model = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(action_size, activation=\"linear\")\n",
    "])\n",
    "print(model.summary())\n",
    "# use Adam to optimize the error\n",
    "optimizer = Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss and the Q value by using MAE based on Bellman equation\n",
    "def compute_loss(experiences, gamma, model, target_model):\n",
    "  states, actions, rewards, next_states, done_vals = experiences\n",
    "  max_qsa = tf.reduce_max(target_model(next_states), axis=-1)\n",
    "  y_targets = rewards + (gamma * max_qsa * (1-done_vals))\n",
    "  q_values = model(states)\n",
    "  q_values = tf.gather_nd(q_values, \n",
    "             tf.stack([tf.range(q_values.shape[0]),\n",
    "             tf.cast(actions, tf.int32)], axis=1))\n",
    "  loss = MSE(y_targets, q_values)\n",
    "  return loss\n",
    "\n",
    "# set up decay rate 0.01 to update the weight of the target network \n",
    "def update_target_network(model, target_model):\n",
    "  TAU=0.01\n",
    "  for target_weights, model_weights in zip(target_model.weights, model.weights):\n",
    "    target_weights.assign(TAU * model_weights + (1.0-TAU) * target_weights)\n",
    "\n",
    "# To compute gradients based the errors we get and update weights\n",
    "def agent_learn(experiences, gamma, model, target_model, optimizer):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(experiences, gamma, model, target_model)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  update_target_network(model, target_model)\n",
    "\n",
    "# randomly choose action if the generated random floating numbers is less than epsilon\n",
    "def get_action(q_values, epsilon=0):\n",
    "  if random.random() > epsilon:\n",
    "    return np.argmax(q_values.numpy()[0])\n",
    "  else:\n",
    "    return random.choice(np.arange(6))\n",
    "\n",
    "# while training, check if training step is finished and wether size of input data is greater than memory_buffer\n",
    "def check_update_conditions(iter, NUM_STEPS_FOR_UPDATE, memory_buffer):\n",
    "  if(iter+1) % NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) > 128:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# record the experience in sequence near its endpoints and \n",
    "# convert to tensor before fedding to exploit\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=64)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "# calculate and get the epsilon after decay\n",
    "def get_new_epsilon(epsilon):\n",
    "  E_MIN = 0.01\n",
    "  E_DECAY = 0.005\n",
    "  return max(E_MIN, E_DECAY * epsilon)\n",
    "\n",
    "# get next state by categorical data array into numerical values\n",
    "def get_one_hot_encoding(state, next_state):\n",
    "  state_arr = np.zeros(500)\n",
    "  next_state_arr = np.zeros(500)\n",
    "  state_arr[state] = 1\n",
    "  next_state_arr[next_state] = 1\n",
    "  return state_arr, next_state_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the condition, train the model and get the average reward\n",
    "def train(NUM_EPISODES, MAX_TIMESTEPS):\n",
    "  # NUM_EPISODES = 1000\n",
    "  # MAX_TIMESTEPS = 100\n",
    "  memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "  target_model.set_weights(model.get_weights())\n",
    "  epsilon = 0.02\n",
    "  points_history = []\n",
    "\n",
    "  for i in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    state, _ = get_one_hot_encoding(state, 0)\n",
    "    total_points = 0\n",
    "\n",
    "    for iter in range(MAX_TIMESTEPS):\n",
    "      state_qn = np.expand_dims(state, axis=0)\n",
    "      q_values = model(state_qn)\n",
    "      action = get_action(q_values, epsilon)\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      _, next_state = get_one_hot_encoding(0, next_state)\n",
    "      memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "      update = check_update_conditions(iter, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "\n",
    "      if update:\n",
    "        experiences = get_experiences(memory_buffer)\n",
    "        agent_learn(experiences, GAMMA, model, target_model, optimizer)\n",
    "      state = next_state.copy()\n",
    "      total_points += reward\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    points_history.append(total_points)\n",
    "    avg_points = np.mean(points_history[-100:])\n",
    "    epsilon = get_new_epsilon(epsilon)\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Total point average of the last 100 episodes: -102.86"
     ]
    }
   ],
   "source": [
    "# call train function to get the result\n",
    "train(1000,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dda6258f530033974f4bfe5304e9c9e642f022d4f7c607bddf33f4fd7a453f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
