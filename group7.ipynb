{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 7 - Yang Songyu, Qi, Yuehan, Kung Jeffrey\n",
    "## The Taxi game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the libraries for the environment\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "%pip install pyvirtualdisplay\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE, MAE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# set up the environment\n",
    "ENV_NAME = \"Taxi-v3\"\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Implementation- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Implementation-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation- Yang Songyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global random seeds\n",
    "tf.random.set_seed(0)\n",
    "# set up the hyperparameter\n",
    "MEMORY_SIZE = 800_000\n",
    "GAMMA = 0.95\n",
    "ALPHA = 0.1\n",
    "NUM_STEPS_FOR_UPDATE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "Number of states: 500\n"
     ]
    }
   ],
   "source": [
    "# create tuple subclasses with named fields\n",
    "experience = namedtuple(\"Experience\", \n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "#Since gym libries already assign the actions and state for the Taxi game\n",
    "#We do not need to assign them again\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "print(\"Number of actions:\", action_size)\n",
    "print(\"Number of states:\",state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 128)               64128     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 97,926\n",
      "Trainable params: 97,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The network contains one input layer which instantiate a Keras tensor, \n",
    "# three hidden dense layer and the final dense layer should have the same size of action space\n",
    "model = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(action_size, activation=\"linear\")\n",
    "])\n",
    "target_model = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(action_size, activation=\"linear\")\n",
    "])\n",
    "print(model.summary())\n",
    "# use Adam to optimize the error\n",
    "optimizer = Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss and the Q value by using MAE based on Bellman equation\n",
    "def compute_loss(experiences, gamma, model, target_model):\n",
    "  states, actions, rewards, next_states, done_vals = experiences\n",
    "  max_qsa = tf.reduce_max(target_model(next_states), axis=-1)\n",
    "  y_targets = rewards + (gamma * max_qsa * (1-done_vals))\n",
    "  q_values = model(states)\n",
    "  q_values = tf.gather_nd(q_values, \n",
    "             tf.stack([tf.range(q_values.shape[0]),\n",
    "             tf.cast(actions, tf.int32)], axis=1))\n",
    "  loss = MSE(y_targets, q_values)\n",
    "  return loss\n",
    "\n",
    "# set up decay rate 0.01 to update the weight of the target network \n",
    "def update_target_network(model, target_model):\n",
    "  TAU=0.01\n",
    "  for target_weights, model_weights in zip(target_model.weights, model.weights):\n",
    "    target_weights.assign(TAU * model_weights + (1.0-TAU) * target_weights)\n",
    "\n",
    "# To compute gradients based the errors we get and update weights\n",
    "def agent_learn(experiences, gamma, model, target_model, optimizer):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(experiences, gamma, model, target_model)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  update_target_network(model, target_model)\n",
    "\n",
    "# randomly choose action if the generated random floating numbers is less than epsilon\n",
    "def get_action(q_values, epsilon=0):\n",
    "  if random.random() > epsilon:\n",
    "    return np.argmax(q_values.numpy()[0])\n",
    "  else:\n",
    "    return random.choice(np.arange(6))\n",
    "\n",
    "# while training, check if training step is finished and wether size of input data is greater than memory_buffer\n",
    "def check_update_conditions(iter, NUM_STEPS_FOR_UPDATE, memory_buffer):\n",
    "  if(iter+1) % NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) > 128:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# record the experience in sequence near its endpoints and \n",
    "# convert to tensor before fedding to exploit\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=64)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "# calculate and get the epsilon after decay\n",
    "def get_new_epsilon(epsilon):\n",
    "  E_MIN = 0.01\n",
    "  E_DECAY = 0.005\n",
    "  return max(E_MIN, E_DECAY * epsilon)\n",
    "\n",
    "# get next state by categorical data array into numerical values\n",
    "def get_one_hot_encoding(state, next_state):\n",
    "  state_arr = np.zeros(500)\n",
    "  next_state_arr = np.zeros(500)\n",
    "  state_arr[state] = 1\n",
    "  next_state_arr[next_state] = 1\n",
    "  return state_arr, next_state_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the condition, train the model and get the average reward\n",
    "def train(NUM_EPISODES, MAX_TIMESTEPS):\n",
    "  # NUM_EPISODES = 1000\n",
    "  # MAX_TIMESTEPS = 100\n",
    "  memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "  target_model.set_weights(model.get_weights())\n",
    "  epsilon = 0.02\n",
    "  points_history = []\n",
    "\n",
    "  for i in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    state, _ = get_one_hot_encoding(state, 0)\n",
    "    total_points = 0\n",
    "\n",
    "    for iter in range(MAX_TIMESTEPS):\n",
    "      state_qn = np.expand_dims(state, axis=0)\n",
    "      q_values = model(state_qn)\n",
    "      action = get_action(q_values, epsilon)\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      _, next_state = get_one_hot_encoding(0, next_state)\n",
    "      memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "      update = check_update_conditions(iter, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "\n",
    "      if update:\n",
    "        experiences = get_experiences(memory_buffer)\n",
    "        agent_learn(experiences, GAMMA, model, target_model, optimizer)\n",
    "      state = next_state.copy()\n",
    "      total_points += reward\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    points_history.append(total_points)\n",
    "    avg_points = np.mean(points_history[-100:])\n",
    "    epsilon = get_new_epsilon(epsilon)\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Total point average of the last 100 episodes: -102.52"
     ]
    }
   ],
   "source": [
    "# call train function to get the result\n",
    "train(1000,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dda6258f530033974f4bfe5304e9c9e642f022d4f7c607bddf33f4fd7a453f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
