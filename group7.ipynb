{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 7 - Yang Songyu, Qi, Yuehan, Kung Jeffrey\n",
    "## The Taxi game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: gym[toy_text]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipykernel in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (6.17.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (7.28.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (7.0.6)\n",
      "Requirement already satisfied: packaging in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (5.1.1)\n",
      "Requirement already satisfied: psutil in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (1.5.1)\n",
      "Requirement already satisfied: appnope in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (0.1.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (23.2.0)\n",
      "Requirement already satisfied: nest-asyncio in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (1.5.5)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipykernel) (0.1.3)\n",
      "Requirement already satisfied: decorator in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: pygments in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (3.0.20)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (65.5.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.18.1)\n",
      "Requirement already satisfied: backcall in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: entrypoints in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from packaging->ipykernel) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the libraries for the environment\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "%pip install pyvirtualdisplay\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE, MAE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "%pip install gym[toy_text]\n",
    "%pip install ipykernel\n",
    "from gym import envs\n",
    "import datetime\n",
    "import keras \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from time import sleep\n",
    "\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "# set up the environment\n",
    "ENV_NAME = \"Taxi-v3\"\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward at Episode 50: -103\n",
      "Average timesteps per episode: 8.633\n",
      "Total Reward at Episode 100: -123\n",
      "Average timesteps per episode: 13.635\n",
      "Total Reward at Episode 150: 9\n",
      "Average timesteps per episode: 16.15\n",
      "Total Reward at Episode 200: 3\n",
      "Average timesteps per episode: 17.786\n",
      "Total Reward at Episode 250: -23\n",
      "Average timesteps per episode: 18.682\n",
      "Total Reward at Episode 300: 11\n",
      "Average timesteps per episode: 19.501\n",
      "Total Reward at Episode 350: 12\n",
      "Average timesteps per episode: 20.395\n",
      "Total Reward at Episode 400: 7\n",
      "Average timesteps per episode: 21.158\n",
      "Total Reward at Episode 450: 7\n",
      "Average timesteps per episode: 21.855\n",
      "Total Reward at Episode 500: 11\n",
      "Average timesteps per episode: 22.524\n",
      "Total Reward at Episode 550: 8\n",
      "Average timesteps per episode: 23.218\n",
      "Total Reward at Episode 600: 9\n",
      "Average timesteps per episode: 23.886\n",
      "Total Reward at Episode 650: 8\n",
      "Average timesteps per episode: 24.582\n",
      "Total Reward at Episode 700: 6\n",
      "Average timesteps per episode: 25.287\n",
      "Total Reward at Episode 750: 10\n",
      "Average timesteps per episode: 25.95\n",
      "Total Reward at Episode 800: 11\n",
      "Average timesteps per episode: 26.62\n",
      "Total Reward at Episode 850: 4\n",
      "Average timesteps per episode: 27.296\n",
      "Total Reward at Episode 900: 3\n",
      "Average timesteps per episode: 27.954\n",
      "Total Reward at Episode 950: 8\n",
      "Average timesteps per episode: 28.561\n",
      "Total Reward at Episode 1000: 4\n",
      "Average timesteps per episode: 29.245\n",
      "Average reward over 1000 episodes: -15.886\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.]\n",
      "[-1.719 -1.719 -1.719 13.59  -9.    -9.   ]\n",
      "Time taken: 29245\n"
     ]
    }
   ],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS]) \n",
    "\n",
    "gamma = 0.9 # discount factor\n",
    "alpha = 0.9 # learning rate\n",
    "epsilon = 0.9\n",
    "\n",
    "total_epochs = 0\n",
    "\n",
    "eps =[]\n",
    "rewd = []\n",
    "\n",
    "for episode in range(1,1001):\n",
    "    done = False\n",
    "    total_rewd = 0 # total reward\n",
    "    obs = env.reset()\n",
    "    epochs = 0\n",
    "    steps = []\n",
    "\n",
    "    while done != True:\n",
    "      if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "      else:\n",
    "            # Exploit learned values\n",
    "            action = np.argmax(Q[obs]) \n",
    "            obs2, rew, done, info = env.step(action) \n",
    "            Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) \n",
    "\n",
    "            total_rewd +=  rew\n",
    "            obs = obs2   \n",
    "            epochs += 1\n",
    "\n",
    "    total_epochs += epochs\n",
    "\n",
    "    eps.append(episode)\n",
    "    rewd.append(total_rewd)\n",
    "    avgsteps = total_epochs / 1000\n",
    "\n",
    "    # print out the result for every 50 episodes\n",
    "    if episode % 50 == 0: \n",
    "      \n",
    "      print('Total Reward at Episode {}: {}'.format(episode,total_rewd))\n",
    "      print(f\"Average timesteps per episode: {avgsteps}\")\n",
    "      \n",
    "\n",
    "avgrewd = sum(rewd) / 1000\n",
    "\n",
    "print(\"Average reward over 1000 episodes:\", avgrewd)\n",
    "\n",
    "plt.plot(eps,rewd)\n",
    "plt.title(\"Rewards for Episodes over time\")\n",
    "plt.ylabel(\"Total Rewards\")\n",
    "plt.xlabel(\"Number of Episodes\")\n",
    "plt.show()\n",
    "\n",
    "# Generating Q-table\n",
    "print(Q[0])\n",
    "print(Q[499])\n",
    "\n",
    "print(\"Time taken:\", total_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | :\u001b[42m_\u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : |\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Reward: 15\n"
     ]
    }
   ],
   "source": [
    "#following the policy to take actions delivering max value\n",
    "\n",
    "total_rewd=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    total_rewd = total_rewd + rew\n",
    "    rewd.append(total_rewd)\n",
    "    env.render()\n",
    "#Print the reward of these actions\n",
    "max_rewd = max(rewd)\n",
    "print(\"Reward: %r\" % max_rewd)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi-v3\n",
      "Action Space 6\n",
      "State Space 500\n",
      "Episode:   1000/1000 || Average reward: -655.67 || eps: 0.010000"
     ]
    }
   ],
   "source": [
    "#agent class\n",
    "class Agent:\n",
    "    def __init__(self, Q, nA=6):\n",
    "        self.Q = Q\n",
    "        self.nA = nA\n",
    "        self.step = self.monteCarloControlStep\n",
    "        self.alpha = 0.01 \n",
    "        self.gamma = 0.9  \n",
    "        self.episode = list()\n",
    "        \n",
    "    #action selection function\n",
    "    def selectAction(self, state, eps):\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(self.Q[state])\n",
    "        else:\n",
    "            return np.random.choice(self.nA)\n",
    "\n",
    "    #monte carlo control step function\n",
    "    def monteCarloControlStep(self, state, action, reward, next_state, done):\n",
    "\n",
    "        if done:\n",
    "            rewards = defaultdict(lambda: np.zeros(self.nA))\n",
    "            for history in reversed(self.episode):\n",
    "                state, action, reward = history\n",
    "                rewards[state][action] = reward + self.gamma * rewards[state][action]\n",
    "                self.Q[state][action] += self.alpha * (rewards[state][action] - self.Q[state][action])\n",
    "            self.episode.clear()\n",
    "        else:\n",
    "            self.episode.append((state, action, reward))\n",
    "\n",
    "#monte carlo control function\n",
    "def monteCarloControl(Q = defaultdict(lambda: np.zeros(action_size))):\n",
    "    agent = Agent(Q)\n",
    "    totalEps = 1000\n",
    "    sampleRewards = deque(maxlen=100)\n",
    "\n",
    "    for episodeI in range(totalEps):\n",
    "        state = env.reset()\n",
    "        eps = 0.01\n",
    "        sampleReward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.selectAction(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            sampleReward += reward\n",
    "            if done:\n",
    "                sampleRewards.append(sampleReward)\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        if episodeI >= 100:\n",
    "            rewardAvg = sum(sampleRewards) / len(sampleRewards)\n",
    "            episodeI += 1\n",
    "            print(f\"\\rEpisode: {episodeI:6d}/{totalEps} || Average reward: {rewardAvg:7.2f} || eps: {eps:.5f}\", end='')\n",
    "\n",
    "\n",
    "            \n",
    "env = gym.make(\"Taxi-v3\")\n",
    "print(env.spec.id)\n",
    "\n",
    "# Number of possible actions\n",
    "action_size = env.action_space.n\n",
    "print(f\"Action Space {env.action_space.n}\")\n",
    "\n",
    "# Number of possible states\n",
    "space_size = env.observation_space.n\n",
    "print(f\"State Space {env.observation_space.n}\")\n",
    "\n",
    "#function call\n",
    "monteCarloControl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global random seeds\n",
    "tf.random.set_seed(0)\n",
    "# set up the hyperparameter\n",
    "MEMORY_SIZE = 800_000\n",
    "GAMMA = 0.95\n",
    "ALPHA = 0.1\n",
    "NUM_STEPS_FOR_UPDATE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "Number of states: 500\n"
     ]
    }
   ],
   "source": [
    "# create tuple subclasses with named fields\n",
    "experience = namedtuple(\"Experience\", \n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "#Since gym libries already assign the actions and state for the Taxi game\n",
    "#We do not need to assign them again\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "print(\"Number of actions:\", action_size)\n",
    "print(\"Number of states:\",state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 128)               64128     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 97,926\n",
      "Trainable params: 97,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The network contains one input layer which instantiate a Keras tensor, \n",
    "# three hidden dense layer and the final dense layer should have the same size of action space\n",
    "model = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(action_size, activation=\"linear\")\n",
    "])\n",
    "target_model = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(action_size, activation=\"linear\")\n",
    "])\n",
    "print(model.summary())\n",
    "# use Adam to optimize the error\n",
    "optimizer = Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss and the Q value by using MAE based on Bellman equation\n",
    "def compute_loss(experiences, gamma, model, target_model):\n",
    "  states, actions, rewards, next_states, done_vals = experiences\n",
    "  max_qsa = tf.reduce_max(target_model(next_states), axis=-1)\n",
    "  y_targets = rewards + (gamma * max_qsa * (1-done_vals))\n",
    "  q_values = model(states)\n",
    "  q_values = tf.gather_nd(q_values, \n",
    "             tf.stack([tf.range(q_values.shape[0]),\n",
    "             tf.cast(actions, tf.int32)], axis=1))\n",
    "  loss = MSE(y_targets, q_values)\n",
    "  return loss\n",
    "\n",
    "# set up decay rate 0.01 to update the weight of the target network \n",
    "def update_target_network(model, target_model):\n",
    "  TAU=0.01\n",
    "  for target_weights, model_weights in zip(target_model.weights, model.weights):\n",
    "    target_weights.assign(TAU * model_weights + (1.0-TAU) * target_weights)\n",
    "\n",
    "# To compute gradients based the errors we get and update weights\n",
    "def agent_learn(experiences, gamma, model, target_model, optimizer):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(experiences, gamma, model, target_model)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  update_target_network(model, target_model)\n",
    "\n",
    "# randomly choose action if the generated random floating numbers is less than epsilon\n",
    "def get_action(q_values, epsilon=0):\n",
    "  if random.random() > epsilon:\n",
    "    return np.argmax(q_values.numpy()[0])\n",
    "  else:\n",
    "    return random.choice(np.arange(6))\n",
    "\n",
    "# while training, check if training step is finished and wether size of input data is greater than memory_buffer\n",
    "def check_update_conditions(iter, NUM_STEPS_FOR_UPDATE, memory_buffer):\n",
    "  if(iter+1) % NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) > 128:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "# record the experience in sequence near its endpoints and \n",
    "# convert to tensor before fedding to exploit\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=64)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "# calculate and get the epsilon after decay\n",
    "def get_new_epsilon(epsilon):\n",
    "  E_MIN = 0.01\n",
    "  E_DECAY = 0.005\n",
    "  return max(E_MIN, E_DECAY * epsilon)\n",
    "\n",
    "# get next state by categorical data array into numerical values\n",
    "def get_one_hot_encoding(state, next_state):\n",
    "  state_arr = np.zeros(500)\n",
    "  next_state_arr = np.zeros(500)\n",
    "  state_arr[state] = 1\n",
    "  next_state_arr[next_state] = 1\n",
    "  return state_arr, next_state_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the condition, train the model and get the average reward\n",
    "def train(NUM_EPISODES, MAX_TIMESTEPS):\n",
    "  # NUM_EPISODES = 1000\n",
    "  # MAX_TIMESTEPS = 100\n",
    "  memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "  target_model.set_weights(model.get_weights())\n",
    "  epsilon = 0.02\n",
    "  points_history = []\n",
    "\n",
    "  for i in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    state, _ = get_one_hot_encoding(state, 0)\n",
    "    total_points = 0\n",
    "\n",
    "    for iter in range(MAX_TIMESTEPS):\n",
    "      state_qn = np.expand_dims(state, axis=0)\n",
    "      q_values = model(state_qn)\n",
    "      action = get_action(q_values, epsilon)\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      _, next_state = get_one_hot_encoding(0, next_state)\n",
    "      memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "      update = check_update_conditions(iter, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "\n",
    "      if update:\n",
    "        experiences = get_experiences(memory_buffer)\n",
    "        agent_learn(experiences, GAMMA, model, target_model, optimizer)\n",
    "      state = next_state.copy()\n",
    "      total_points += reward\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "    points_history.append(total_points)\n",
    "    avg_points = np.mean(points_history[-100:])\n",
    "    epsilon = get_new_epsilon(epsilon)\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 | Total point average of the last 100 episodes: -108.64"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q3/0rjm6gb921s0ghz13gkws7kw0000gn/T/ipykernel_72050/306357295.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# call train function to get the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q3/0rjm6gb921s0ghz13gkws7kw0000gn/T/ipykernel_72050/922988428.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(NUM_EPISODES, MAX_TIMESTEPS)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mtotal_points\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q3/0rjm6gb921s0ghz13gkws7kw0000gn/T/ipykernel_72050/1559616087.py\u001b[0m in \u001b[0;36magent_learn\u001b[0;34m(experiences, gamma, model, target_model, optimizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1692\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5525\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5527\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5528\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5529\u001b[0m         transpose_b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# call train function to get the result\n",
    "train(1000,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dda6258f530033974f4bfe5304e9c9e642f022d4f7c607bddf33f4fd7a453f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
